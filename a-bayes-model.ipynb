{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530e9d41-7293-4833-b151-5d3cd8cb4e39",
   "metadata": {},
   "source": [
    "# Une analyse naïve des sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d408dd4-ad07-48aa-9c53-ca148f243c21",
   "metadata": {},
   "source": [
    "Vous travaillez pour un voyagiste (*tour operator*) qui souhaite automatiser l’analyse des avis de ses clients. Pour mener à bien votre tâche, vous allez entraîner un classifieur bayésien naïf sur un jeu de données fictif afin de déterminer automatiquement, à partir d’un nouveau message, si l’avis exprimé est positif ou négatif.\n",
    "\n",
    "Si les hypothèses derrière les modèles bayésiens naïfs sont très rarement vérifiées dans la réalité, ce type d’algorithme probabiliste montre malgré tout de très bons résultats. Au nombre de ses avantages, il ne nécessite en plus que très peu de données et les calculs requis sont relativement simples à implémenter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e124610-4c93-4ea8-9d1d-be8b69818be8",
   "metadata": {},
   "source": [
    "## Présentation des ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb54257-024c-4595-a309-99ba389bf695",
   "metadata": {},
   "source": [
    "Dans ce dossier, vous trouverez :\n",
    "\n",
    "- un répertoire *corpus* avec deux sous-répertoires *positive* et *negative* qui contiennent chacun une dizaine de messages classés comme globalement positifs ou négatifs ;\n",
    "- un répertoire *data* avec :\n",
    "   - un fichier *SWN.txt* qui est une extraction appauvrie de SentiWordNet 3.0 ;\n",
    "   - deux fichiers de fréquences *positive.txt* et *negative.txt* enrichis des scores de positivité et de négativité de *SWN.txt* ;\n",
    "   - un fichier *vocabulary.txt* qui représente le lexique global pondéré du corpus d’apprentissage ;\n",
    "- et enfin un répertoire *test* qui contient la modélisation des deux nouveaux messages à classer.\n",
    "\n",
    "**Remarque :** Tous les messages ont été générés par ChatGPT afin qu’ils ressemblent à des commentaires écrits par des touristes en visite dans un pays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ad02d-dc5e-405e-8745-ae206f20b9c5",
   "metadata": {},
   "source": [
    "### SentiWordNet, une ressource lexicale pour l’analyse de sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe84a33-5d9d-4ab7-b4c0-889b3d3ff340",
   "metadata": {},
   "source": [
    "[*SentiWordNet 3.0*](https://github.com/aesuli/SentiWordNet) est une base de données lexicale utilisée en analyse de sentiments (*opinion mining*). Elle associe à chaque synset (ensemble de synonymes) de [WordNet 3.0](https://wordnet.princeton.edu/) trois scores numériques représentant les polarités sentimentales : positif, négatif et objectif (neutre).\n",
    "\n",
    "Chaque score varie entre 0 et 1 et reflète l’**intensité intrinsèque** du sentiment associé au synset, c’est-à-dire une mesure statique qui ne dépend pas du contexte d’utilisation du mot. Autrement dit, ces scores sont attribués au sens précis du mot dans le synset, indépendamment de la phrase ou du document dans lequel il apparaît.\n",
    "\n",
    "Cette ressource permet d’évaluer automatiquement le sentiment exprimé dans des textes en s’appuyant sur la signification des mots, ce qui ajoute une dimension **sémantique** à l’analyse et facilite ainsi des tâches comme la classification d’avis ou l’analyse d’opinion.\n",
    "\n",
    "Le fichier *SWN.txt* ne récupère que les scores de positivité et de négativité des couples lemmes/étiquettes trouvés dans les avis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9f92b-e8bb-4a5e-a074-e47b6ab093d3",
   "metadata": {},
   "source": [
    "#### Quelques notions de linguistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e3ae9-8eb0-45c1-a6d8-3fdee68aa57e",
   "metadata": {},
   "source": [
    "**Lemme :** en linguistique, le lemme désigne la forme canonique d’un mot. En traitement automatique d’un texte, l’opération de lemmatisation intervient après la segmentation en mots. C’est une phase importante qui demande d’être sensibilisé aux questions de désambiguïsation, car un même mot peut avoir plusieurs formes fléchies (pluriel, conjugaison, genre) et plusieurs sens. La lemmatisation consiste à ramener chaque forme fléchie à sa forme de base pour faciliter l’analyse.\n",
    "\n",
    "**Étiquette en partie du discours (POS tag) :** C’est une annotation qui indique la catégorie grammaticale d’un mot dans une phrase, comme nom, verbe, adjectif, adverbe, etc. Cette étape permet de mieux comprendre le rôle syntaxique de chaque mot et est essentielle pour des tâches telles que la désambiguïsation, la reconnaissance d’entités nommées ou l’analyse syntaxique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb2d76-e32c-4c24-8ad7-e5f1958a2944",
   "metadata": {},
   "source": [
    "### Un lexique pondéré ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8adf8-fe35-4cfc-9670-05239d168839",
   "metadata": {},
   "source": [
    "Le fichier *vocabulary.txt* a été établi à partir des fichiers *negative.txt* et *positive.txt* afin de ressortir une mesure combinée de la polarité lexicale et de la fréquence d’occurrence.\n",
    "\n",
    "Les colonnes *posWeight* et *negWeight* représentent ainsi une quantification numérique de la polarité du couple (lemme, étiquette) dans le contexte d’apprentissage. Elle a été mesurée avec la formule suivante :\n",
    "\n",
    "$$\n",
    "\\text{weight}_{y}(x) = \\frac{(F_{y}(x) + \\alpha) \\cdot \\text{score}_{y}(x)}{\\sum\\limits_{x' \\in V} \\text{score}_{y}(x')}\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $x$ est un lemme du vocabulaire $V$ ;\n",
    "- $F_{y}(x)$ est la fréquence du lemme pour la classe sachant que $y \\in \\{\\text{+}, \\text{-}\\}$ ;\n",
    "- $\\alpha$ est une constante de lissage fixée à 0,5 ;\n",
    "- $\\text{score}_{y}(x)$ est le *posScore* ou le *negScore* selon la classe $y$ considérée.\n",
    "\n",
    "Cette formule garantit d’une part que toutes les valeurs sont positives, un pré-requis des modèles bayésiens naïfs multinomiaux, et d’autre part qu’un lemme marqué par une polarité qui n’apparaîtrait pas dans un contexte ou l’autre conserve un poids malgré tout supérieur à zéro. Un exemple : le mot *beautifully* jouit d’un *posScore* de 0,375 mais n’apparaît pas dans le sous corpus positif. Sans le lissage de 0,5, le poids du mot en contexte d’apprentissage aurait été nul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4462a7-2236-4c7a-b9fb-4c7b4e8e1421",
   "metadata": {},
   "source": [
    "## Un classifieur naïf bayésien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887a24f-b917-4282-a803-4d674d77688c",
   "metadata": {},
   "source": [
    "L’algorithme de classification naïve bayésienne, largement utilisé en intelligence artificielle, repose sur le théorème de Bayes pour prédire la classe d’une donnée tout en supposant l’indépendance de ses caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f35b2c-982c-40a2-a0cd-21a03b0bd75b",
   "metadata": {},
   "source": [
    "### Hypothèse d’indépendance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a7dda-c90c-4686-81d1-618127564971",
   "metadata": {},
   "source": [
    "#### Une hypothèse bien naïve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d66b40-a8ec-4df2-8d77-0ace67021465",
   "metadata": {},
   "source": [
    "La taille d’un chat a-t-elle une influence sur la longueur de son appendice caudal ? Le nombre d’heures de sommeil joue-t-il une quelconque importance dans l’état de vigilance d’une personne ? Le nombre d’enfants est-il en relation avec le type de la voiture qu’un foyer possède ?\n",
    "\n",
    "À raison, on aurait tendance à répondre positivement à toutes ces questions, mais pas un classifieur naïf bayésien. Pour lui, aucune caractéristique n’influence une autre. Bien que cette hypothèse soit irréaliste dans la plupart des cas, elle a prouvé sa solidité en termes de résultats. Qui plus est, elle permet de simplifier les calculs et d’obtenir des modèles légers et rapides, ce qui en fait une méthode particulièrement efficace lorsque la quantité de données est faible ou que les ressources technologiques sont limitées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31e675-8621-418d-824b-b40bed74ef3f",
   "metadata": {},
   "source": [
    "#### Application à une tâche de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d5abf-a49d-4a31-ab86-7d85f2055e6c",
   "metadata": {},
   "source": [
    "Une tâche de classification a pour objectif d’associer à un objet une classe ($y$) en fonction de caractéristiques ($X$). On peut le traduire en termes de probabilités d’obtenir $y$ sachant $X$ :\n",
    "\n",
    "$$\n",
    "P(y \\mid X) = \\frac{P(X \\cap y)}{P(X)} = \\frac{P(X \\mid y) \\cdot P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "Comme $X$ est une matrice de facteurs ($x_1 \\,, x_2 \\,, \\dots \\,, x_n$) indépendants les uns des autres, la formule peut s’exprimer avec un produit de conditions indépendantes :\n",
    "\n",
    "$$\n",
    "P(y \\mid x_1 \\,, x_2 \\,, \\dots \\,, x_n) = \\frac{P (y) \\times \\prod_{i=1}^n P(x_{i} \\mid y)}{\\prod_{i=1}^n P(x_{i})}\n",
    "$$\n",
    "\n",
    "Et comme pour toutes les observations du jeu de données le dénominateur est constant, on peut le supprimer en inférant une notion de proportionnalité :\n",
    "\n",
    "$$\n",
    "P(y \\mid x_{1} \\,, x_{2} \\,, \\dots \\,, x_n) \\propto P (y) \\times \\prod_{i=1}^n P(x_{i} \\mid y)\n",
    "$$\n",
    "\n",
    "La fonction prédictive est appliquée à l’ensemble des classes possibles et le maximum a posteriori (MAP) sert ensuite à élire la classe la plus plausible :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_{y \\in \\{C_1, C_2, \\dots, C_K\\}} P(y) \\cdot \\prod_{i=1}^n P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "**Remarque :** Si les facteurs n’avaient pas été jugés indépendants, la **règle du produit** qui exprime des probabilités conjointes sous forme de produits de probabilités conditionnelles entraînerait des calculs bien plus complexes :  \n",
    "\n",
    "$$\n",
    "P(x_1 \\,, x_2 \\,, \\dots \\,, x_n \\mid y) = P(x_1 \\mid y) \\times P(x_2 \\mid x_1 \\cap y) \\times P(x_3 \\mid x_1 \\cap x_2 \\cap y) \\times \\dots \\times P(x_n \\mid x_1 \\cap x_2 \\cap \\dots \\cap x_{n-1} \\cap, y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1c234-6652-45a2-b8e7-23f500d8b96f",
   "metadata": {},
   "source": [
    "### Les types de classifieurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd977698-0f43-4ab0-b97d-3a5f860476d0",
   "metadata": {},
   "source": [
    "On distingue trois catégories de classifieurs :\n",
    "\n",
    "- **Le naïf bayésien multinomial :** utilisé principalement pour les données discrètes, comme la classification de texte où l’on comptabilise les occurrences de mots.\n",
    "- **Le naïf bayésien gaussien :** utilisé lorsque les données sont continues et supposent une distribution normale pour chaque caractéristique au sein de chaque classe. Si ce n’est pas le cas, une transformation peut être appliquée, voire un regroupement en classes, mais c’est au prix d’une perte sans doute conséquente d’information.\n",
    "- **Le naïf bayésien de Bernoulli :** adapté aux données binaires comme lorsque l’on signale la présence ou l’absence d’un mot plutôt que de compter ses occurrences.\n",
    "\n",
    "En pratique, il est rare qu’un jeu de données ne présente qu’un seul type de variables, aussi on adopte plutôt une approche hybride où chaque variable est modélisée conformément à sa distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf72c0d-2f24-463e-bd15-7fdd8a04f3e9",
   "metadata": {},
   "source": [
    "## Étape 1 : Calculer la probabilité conditionnelle de chaque lemme selon la classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7cefe-3bdc-4660-923e-9eac03177840",
   "metadata": {},
   "source": [
    "Nous souhaitons connaître à présent la probabilité d’apparition de chaque lemme selon le contexte. Il s’agit de calculer pour un lemme :\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{F(x_i, y)}{F(y)}\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $F(x_i \\,, y)$ est la fréquence du lemme $x_i$ dans la classe envisagée ;\n",
    "- $F(y)$ est la fréquence totale de la classe envisagée dans l’ensemble des données.\n",
    "\n",
    "Par exemple, considérons que le lemme *be* en tant que verbe apparaît dans la classe positive 20 fois sur 130 :\n",
    "\n",
    "$$\n",
    "P(\\text{be} \\mid +) = \\frac{F(\\text{be}, +)}{F(+)} = \\frac{20}{130} = 0,1538\n",
    "$$\n",
    "\n",
    "**Attention !** Comme notre objectif est d’entraîner un classifieur bayésien naïf, nous devons prévoir dans notre modèle le cas où une probabilité serait nulle dans l’une des deux classes, ce qui est d’ailleurs assez souvent le cas dans nos données. Pour éviter l’écueil des probabilités nulles, ajoutez un lissage de Laplace aux probabilités. la formule devient :\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{F(x_i, y) + 1}{F(y) + V}\n",
    "$$\n",
    "\n",
    "Avec $V$ pour la taille du vocabulaire. Pour notre exemple, en donnant $V = 180$, nous obtenons :\n",
    "\n",
    "$$\n",
    "P(\\text{be} \\mid +) = \\frac{F(\\text{be}, +) + 1}{F(+) + V} = \\frac{21}{130 + 180} = 0,0677\n",
    "$$\n",
    "\n",
    "Pour réaliser cette première étape, nous avons donc besoin de calculer $F(+)$, $F(-)$ et $V$.\n",
    "\n",
    "**Pourquoi un lissage ?** Le modèle probabiliste que nous construisons repose sur la multiplication de probabilités. Si un lemme n’apparaît jamais dans un contexte, sa probabilité d’apparition dans ce contexte sera de 0. De là, même si tous les autres lemmes avaient une probabilité de 1 (ils apparaissent systématiquement dans le contexte), on aurait à résoudre $1 \\times 1 \\dots \\times 1 \\times 0$ ce qui reviendrait à un résultat improbable de 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55a2c1-6cdb-4081-8cde-57c0d95dd480",
   "metadata": {},
   "source": [
    "### Calculer la fréquence de chaque classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9cf960-4f39-4a1a-b3a2-61e5a7caf9cc",
   "metadata": {},
   "source": [
    "Le code ci-dessous construit un *data frame* à partir du fichier *vocabulary.txt* et effectue la somme des quantités de la colonne *posFreq* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88512cd4-c7dc-4e65-990c-1b0f4ae80a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file with tab separator\n",
    "df = pd.read_csv('data/vocabulary.txt', sep='\\t')\n",
    "\n",
    "# Calculate the sum of all values in the 'frequencies' column\n",
    "pos_sum = df['posFreq'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebdf10-299f-41d5-8c9b-5c9b4e96095f",
   "metadata": {},
   "source": [
    "Confirmez, en affichant `pos_sum` qu’il y a bien 359 lemmes dans les avis positifs.\n",
    "\n",
    "**Astuce :** Vous pouvez afficher le *data frame* avec les fonctions `display(df_pos)` ou `print(df_pos)`, ou simplement avec l’instruction `df_pos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c4a68-4c7a-41d5-b47e-de5c096e6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48cf982-4b6b-4544-bb80-7071d0be13b5",
   "metadata": {},
   "source": [
    "Essayez à présent de créer une variable `neg_sum` pour les fréquences totales des lemmes en contexte négatif :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef90f53-9b03-4c2c-ba6d-8a8be7103aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80547ec4-c929-44b8-b26e-044516a7516d",
   "metadata": {},
   "source": [
    "### Calculer la taille du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f28854-7d03-4710-af68-ea203598fe97",
   "metadata": {},
   "source": [
    "À présent, vous devriez être en mesure d’enregistrer dans une variable `V` la taille du vocabulaire et trouver un total de 680 lemmes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81cb515-96e9-421a-9e9f-686ede7c73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a28409-4939-4c3c-8b1a-e2f61012a2eb",
   "metadata": {},
   "source": [
    "### Déterminer les probabilités conditionnelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db84cb0-996f-4cb8-b693-fd02ad9f6f8d",
   "metadata": {},
   "source": [
    "Maintenant que vous êtes en possession de tous les termes nécessaires, vous allez ajouter dans le *data frame* les probabilités d’apparition de chaque lemme.\n",
    "\n",
    "**Rappel :** La formule conditionnelle que nous cherchons à implémenter est :\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{F(x_i, y) + 1}{F(y) + V}\n",
    "$$\n",
    "\n",
    "Intégrez en premier lieu la fonction de calcul des probabilités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cca08d-a1a4-4051-be05-513fe780adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the probability function with a Laplacian smoothing\n",
    "def P(frequencies, F, V):\n",
    "    return (frequencies + 1) / (F + V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6096c0d0-739f-4502-813d-d665affc4ac6",
   "metadata": {},
   "source": [
    "L’instruction ci-dessous implémente la fonction pour les lemmes en contexte positif :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a604bb-a805-4520-9165-75b8097c5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posFreq'].apply(lambda f: P(f, pos_sum, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a7555-ffa4-4078-b9c6-ddeb2a8116c4",
   "metadata": {},
   "source": [
    "Assignez le résultat à une variable `ppos` et répétez l’opération pour disposer également d’une variable `pneg` pour les lemmes en contexte négatif :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d39ff2-1810-48fe-ab47-fd5236f3999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d9f6e-0036-437e-8922-1871ee20f08d",
   "metadata": {},
   "source": [
    "Vous pouvez enfin rajouter une colonne à `df` en exécutant l’instruction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bde892-11f2-4843-98d7-ce1049294930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posProb'] = ppos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36afb2-7bae-4637-8e06-6e319a5420ef",
   "metadata": {},
   "source": [
    "Faites de même pour les probabilités en contexte négatif !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371c6a5-5629-4d84-a43a-e74a12afe2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db5e1f-0fc7-4a7e-ae9b-4aa5e8d9be8c",
   "metadata": {},
   "source": [
    "## Étape 2 : Pondérer les fréquences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032296b-a0b8-4ae1-ad46-4fb6f5777d22",
   "metadata": {},
   "source": [
    "Le modèle est déjà bien avancé mais, à ce stade, nous avons calculé des probabilités sans tenir compte des indications sémantiques de *SWN* à travers les coefficients *posScore* et *negScore*.\n",
    "\n",
    "Comment faire pour les inclure ? Ces scores sont conçus pour refléter la polarité intrinsèque d’un mot indépendamment de tout contexte. En conséquence, appliquer directement la pondération sur les fréquences permet de mieux capturer l’importance de chaque mot dans la classification. Une autre solution aurait été de pondérer les probabilités déjà ajustées par le lissage, mais au prix d’un risque de distorsion : des mots fortement pondérés mais peu probables verraient leur importance relevée artificiellement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866312e-2b05-4f71-a7dc-b6e7f84b2067",
   "metadata": {},
   "source": [
    "### Quelle justification à la pondération ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5c2b4-d40a-460f-986f-b8aec1f4f7e4",
   "metadata": {},
   "source": [
    "Le poids agit alors comme un facteur de confiance ou **d’intensité de polarité**. Par exemple, l’adjectif *excellent* véhicule plus de polarité que *good*. Pondérer $P(x_i \\mid y)$ revient ainsi à affirmer que cette probabilité est plus fiable si la polarité est forte. Un autre effet positif de la pondération est qu’elle améliore souvent les classifieurs naïfs en évitant que les mots fréquents mais sémantiquement neutres dominent. La vraisemblance des mots est ainsi corrigée par un signal de polarité empirique très utile en classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e61ba3-941f-480e-a210-eff977a2a11c",
   "metadata": {},
   "source": [
    "### La méthode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174cb62a-8264-4ff4-b67b-d3eb5e42c0f1",
   "metadata": {},
   "source": [
    "Le fichier *vocabulary.txt* expose déjà des colonnes *posWeight* et *negWeight* qui consignent le poids à considérer. Nous pouvons maintenant adapter notre modèle :\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{F(x_i, y) + 1}{F(y) + V} \\times w_i\n",
    "$$\n",
    "\n",
    "Vous pouvez ajouter à présent une colonne *posProb_w* au *data frame* pour enregistrer la pondération :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a57b69-f579-4887-a867-2a5e0097224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe700b5-f7bb-4991-ae44-bb2540905f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4878c70a-a059-4877-b13c-52fde790a3b9",
   "metadata": {},
   "source": [
    "Répétez les opérations pour pondérer les probabilités en contexte négatif :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da25d92-4621-4d7e-828a-5783de37ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b704bbe-5a26-40ca-ae84-477cac3fce94",
   "metadata": {},
   "source": [
    "### Note sur la cohérence de la probabilité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fe7d9-46f2-4d1b-993c-6dba34b62960",
   "metadata": {},
   "source": [
    "Pour garder la cohérence probabiliste et obtenir une vraie distribution de probabilité, il faudrait normaliser en pondérant ainsi :\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{(F(x_i, y) + 1) \\cdot w_i}{\\sum\\limits_j (F(x_j, y) + 1) \\cdot w_j}\n",
    "$$\n",
    "\n",
    "Cette formule garantit que la somme des probabilités sur tous les lemmes $x_j$ est égale à 1, ce qui est essentiel pour une interprétation probabiliste rigoureuse. Cependant, dans de nombreuses applications pratiques, notamment en classification naïve bayésienne, on cherche simplement à comparer les scores relatifs des classes via un $\\arg\\max$. Dans ce contexte, la normalisation peut être omise car elle ne change pas l’ordre relatif des scores. On peut alors utiliser une version non normalisée comme :\n",
    "\n",
    "$$\n",
    "P'(x_i \\mid y) = \\frac{F(x_i, y) + 1}{F(y) + V} \\times w_i\n",
    "$$\n",
    "\n",
    "Cette pondération simplifie les calculs sans perdre en performance pour la décision finale, une flexibilité somme toute courante en apprentissage automatique et en traitement du langage naturel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c938235-d8b5-4bb4-a41b-aa220f0d05a3",
   "metadata": {},
   "source": [
    "## Étape 3 : Calculer la probabilité a priori des classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b21f6-e6c8-4d66-982b-f953e1b86dc5",
   "metadata": {},
   "source": [
    "Il s’agit à présent de calculer $P(+)$ et $P(-)$ qui interviennent dans la formule de Bayes. De nombreuses approches sont envisageables. Nous en retenons trois qui définissent notre connaissance *a priori*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83095bcb-5911-4bc8-9fe3-fcd284804df8",
   "metadata": {},
   "source": [
    "### L’approche naïve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb1166-4553-4e74-a2a2-a3621ab6f609",
   "metadata": {},
   "source": [
    "Une première approche serait de faire le rapport entre le nombre de textes dans chaque contexte et leur nombre total, ce qui, comme le corpus d’apprentissage est équilibré (10 textes dans chaque classe), reviendrait à ne favoriser aucune classe a priori :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(+) &= \\frac{F(+)}{F(+) + F(-)} = \\frac{10}{20} = 0,5\\\\\n",
    "P(-) &= \\frac{F(-)}{F(+) + F(-)} = \\frac{10}{20} = 0,5\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Si vous validez cette approche, vous pouvez exécuter la cellule suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d136b2-8491-4c1a-93e3-13bd64e0e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pos = 0.5\n",
    "prior_neg = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1a0c3-43b1-4a85-a302-be949047470d",
   "metadata": {},
   "source": [
    "### L’approche fréquentiste brute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca325e-6d55-4a9c-99b7-8e8d5826b603",
   "metadata": {},
   "source": [
    "Une autre approche consiste à estimer les probabilités de chaque classe en tenant compte de la distribution des occurrences de mots dans chacune d’elles. \n",
    "\n",
    "Si l’on admet qu’il y a 130 occurrences de mots en contexte positif et 150 en contexte négatif, alors :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(+) &= \\frac{\\sum F(x_{+})}{\\sum F(x_{+}) + \\sum F(x_{-})} = \\frac{130}{280} \\approx 0{,}4643 \\\\\n",
    "P(-) &= \\frac{\\sum F(x_{-})}{\\sum F(x_{+}) + \\sum F(x_{-})} = \\frac{150}{280} \\approx 0{,}5357\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Cette estimation reflète la fréquence totale des mots observés dans chaque classe, et permet d’ajuster la probabilité a priori selon la distribution lexicale effective dans les corpus.\n",
    "\n",
    "Si vous retenez cette approche, et sachant que vous disposez déjà de la somme des lemmes en contexte positifs et négatifs, calculez `prior_pos` et `prior_neg` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf474bb0-43ed-44f1-b0f8-f15432c8fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75185333-cfdf-4f43-bb22-da9268ef64a4",
   "metadata": {},
   "source": [
    "### L’approche cohérente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efaf4b-0ee7-4d00-a567-98c927d7ea79",
   "metadata": {},
   "source": [
    "Et pour rester cohérent·es avec la pondération de chaque mot par leur score de sentiment, vous pourriez l’inclure aussi dans la formule :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(+) &= \\frac{\\sum_i (F(x_{+,i}) \\times w_i)}{\\sum_i (F(x_{+,i}) \\times w_i) + \\sum_j (F(x_{-,j}) \\times w_j)}\\\\\n",
    "P(-) &= \\frac{\\sum_j (F(x_{-,j}) \\times w_j)}{\\sum_i (F(x_{+,i}) \\times w_i) + \\sum_j (F(x_{-,j}) \\times w_j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Cette version pondérée permet de prendre en compte non seulement la fréquence des mots, mais aussi leur importance lexicale via leur poids associé.\n",
    "\n",
    "Pour la calculer, nous avons besoin de connaître au préalable la somme des poids en contextes positifs et négatifs.\n",
    "\n",
    "**Attention !** Afin de bien prendre en considération la contribution d’un lemme aux probabilités de sa classe, nous veillons à multiplier son poids par le nombre de ses apparitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77a9cd-9f66-4825-a75e-521df79f3c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weighted = (df['posFreq'] * df['posWeight']).sum()\n",
    "neg_weighted = (df['negFreq'] * df['negWeight']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eed91c-84bb-4fed-b7db-f42c78769cbb",
   "metadata": {},
   "source": [
    "Il ne reste plus qu’à calculer `prior_pos` et `prior_neg` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce3808-a252-47e1-bc27-085f390fa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23108c6f-b597-4c24-b470-3d238e369b6c",
   "metadata": {},
   "source": [
    "## Étape 4 : Appliquer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e561d-011d-483b-9f72-5ae7b93130bf",
   "metadata": {},
   "source": [
    "### Les commentaires à évaluer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f00c82-2db6-4f98-bfd9-43978193709d",
   "metadata": {},
   "source": [
    "Vous recevez deux nouveaux commentaires :\n",
    "\n",
    ">« The splendid medieval castle stands beautifully preserved on top of a natural landscape, reflecting centuries of fascinating history. The knowledgeable guide made our tour truly unforgettable with her charming storytelling and helpful explanations about the architecture. The exquisite surroundings and breathtaking views from the walls created such a profound sense of awe that I would highly recommend this experience to any history lover. »\n",
    "\n",
    ">« The museum's architecture is interesting and hard to ignore, with clear natural lighting that makes some exhibits seem challenging to appreciate. The historical significance of this place is well-preserved, though walking through the quiet halls feels surprisingly exhausting after a while. While the collections are worth seeing for their profound cultural importance, the overall experience might make visitors feel comfortable or mediocre depending on their expectations. »\n",
    "\n",
    "Dans le répertoire *test*, les fichiers *message_1.txt* et *message_2.txt* contiennent la liste des lemmes des deux messages avec leur fréquence d’occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b076c8-51f2-4606-9507-5054053f1781",
   "metadata": {},
   "source": [
    "### Affinage du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13485cf-78ae-4cb3-9337-7e4b51273651",
   "metadata": {},
   "source": [
    "La formule de prédiction retenue par application du théorème de Bayes était :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_{y \\in \\{C_1, C_2, \\dots, C_K\\}} P(y) \\cdot \\prod_{i=1}^n P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "En pratique, face aux problèmes soulevés par les petites probabilités, on calcule le maximum de vraisemblance d’une classe grâce au logarithme :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_{y \\in \\{C_1, C_2, \\dots, C_K\\}} \\log \\left( P(y) \\times \\prod_{i=1}^n P(x_i \\mid y) \\right)\n",
    "$$\n",
    "\n",
    "En utilisant la propriété multiplicative des logarithmes (le produit devient une somme) :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_{y \\in \\{C_1, C_2, \\dots, C_K\\}} \\left[ \\log P(y) + \\sum_{i=1}^n \\log P(x_i \\mid y) \\right]\n",
    "$$\n",
    "\n",
    "Et, afin de conserver la contribution totale d’un lemme à la prédiction, nous multiplions sa log-probabilité par sa fréquence, représentée par $F(x_i)$ dans la formule :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_{y \\in \\{C_1, C_2, \\dots, C_K\\}} \\left[ \\log P(y) + \\sum_{i=1}^n F(x_i) \\cdot \\log P(x_i \\mid y) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b40b6-1f6a-4337-91b9-a20800e45573",
   "metadata": {},
   "source": [
    "#### Pourquoi les petites probabilités posent-elles problème ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b785d-7de7-4a42-be74-bf85d93c8b1e",
   "metadata": {},
   "source": [
    "Quand on multiplie des probabilités (toutes entre 0 et 1), on obtient rapidement des nombres minuscules :\n",
    "\n",
    "$$\n",
    "0,1 \\times 0,2 \\times 0,05 \\times 0,3 = 0,0003\n",
    "$$\n",
    "\n",
    "Avec 50 probabilités de 0,1 chacune, on obtient par exemple : $0,1^{50} \\approx 10^{-50}$\n",
    "\n",
    "Les ordinateurs ne peuvent pas représenter des nombres aussi petits avec précision, ce qui provoque des erreurs d’arrondi ou des *underflows* (le nombre devient 0).\n",
    "\n",
    "Une manière simple de se rendre compte de la difficulté de la tâche est de demander aux ordinateurs de calculer une exponentiation basique :\n",
    "\n",
    "$$\n",
    "0,1^2 = 0,1 \\times 0,1 = 0,01\n",
    "$$\n",
    "\n",
    "Évaluez le code ci-dessous pour vous convaincre qu’on n’envoie pas Thomas Pesquet sur la lune avec Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395f355-7432-47a4-8623-fc77c609d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c7f16-36c6-4fb7-b136-36becb231f2f",
   "metadata": {},
   "source": [
    "Dans notre cas d’étude, $P(x_i \\mid y)$ sera en effet souvent très proche de zéro aussi le produit de toutes les probabilités aboutira à une valeur si proche de zéro que l’ordinateur aura du mal à la manipuler. Le logarithme présente alors l’avantage de transformer le produit en somme, ce qui facilite les calculs et est en plus numériquement plus stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b9b3c-fcda-48af-aa28-03d580faace9",
   "metadata": {},
   "source": [
    "#### Note sur le logarithme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f3bce-13aa-4acf-99a1-9a657c7113d7",
   "metadata": {},
   "source": [
    "En termes simples, le logarithme répond à la question : « À quelle puissance dois-je élever ma base pour obtenir ce nombre ? »\n",
    "\n",
    "Par exemple, dans un système binaire, on établit que $2^3 = 8$. De là, on sait que le logarithme en base 2 de 8 est 3. On notera : $\\log_2(8) = 3$.\n",
    "\n",
    "En fonction du contexte, on est amené·es à manipuler des bases différentes. En plus du logarithme binaire, citons le logarithme décimal et le logarithme naturel qui utilise la base $e \\approx 2,718$.\n",
    "\n",
    "L’une des propriétés intéressantes du logarithme est qu’il permet de transformer un produit en somme grâce à la relation fondamentale suivante :\n",
    "\n",
    "$$\n",
    "\\log(a \\times b) = \\log(a) + \\log(b)\n",
    "$$\n",
    "\n",
    "En *machine learning*, on utilise couramment le logarithme naturel pour trois raisons principales :\n",
    "\n",
    "1. **Simplification des dérivées :** $\\frac{d}{dx}[\\ln(x)] = \\frac{1}{x}$ ;\n",
    "2. **Optimisation :** Les algorithmes de descente de gradients convergent mieux ;\n",
    "3. **Entropie :** Les mesures d’information utilisent naturellement $\\ln$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f714a3-61d1-47cc-8a43-93b79edd8a15",
   "metadata": {},
   "source": [
    "### Résoudre la tâche de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e482298-8b64-4d51-b603-5cededac78be",
   "metadata": {},
   "source": [
    "Il est maintenant temps de passer à la prédiction. Votre modèle étant entraîné, vous disposez de tous les éléments nécessaires pour estimer la classe $\\hat y$ d’un nouvel exemple, à l’aide de la règle du **maximum a posteriori** (MAP), propre au classifieur bayésien naïf :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname*{argmax}_{y \\in \\{C_1, C_2, \\dots, C_K\\}} \\left[ \\log P(y) + \\sum_{i=1}^n F(x_i) \\cdot \\log P(x_i \\mid y) \\right]\n",
    "$$\n",
    "\n",
    "À l’étape précédente, vous avez en effet déterminé $P(y)$ et, précédemment, vous avez calculé $P(x_i \\mid y)$ soit toutes les probabilités conditionnelles d’apparition des lemmes en fonction de la classe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ea053-9daf-44b9-8c2e-e972a1fd7d2d",
   "metadata": {},
   "source": [
    "#### Étape 1 : Charger les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3ad12-51ba-4885-823c-3a61cd71ba81",
   "metadata": {},
   "source": [
    "Instanciez une variable `df_msg` qui charge les données du fichier *message1.txt* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdf8af-c85b-4838-8cf3-060def7cf9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f9774-c261-4612-983f-0bfe288338b5",
   "metadata": {},
   "source": [
    "Il s’agit maintenant de charger les probabilités conditionnelles des lemmes sachant la classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f2303-34c4-4314-bccb-81c0c4457117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msg = pd.merge(\n",
    "    df_msg,\n",
    "    df[['lemma', 'tag', 'posProb_w', 'negProb_w']],\n",
    "    on=['lemma', 'tag'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76031445-1f7c-4804-91ff-7eafa625f52e",
   "metadata": {},
   "source": [
    "#### Étape 2 : Vérifier la consistance du modèle bayésien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea21bbe2-420c-43f3-8d86-a65c152f59cc",
   "metadata": {},
   "source": [
    "Pour les lemmes du message qui n’apparaissent pas dans le corpus d’apprentissage, nous avons qu’ils pouvaient être facilement ignorés puisqu’ils n’apportent aucune information :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9974e-c2fe-401d-b2e3-415cb044bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msg = df_msg.dropna(subset=['negProb_w', 'posProb_w'], how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5fe750-005b-4adc-9602-34e1cd482d80",
   "metadata": {},
   "source": [
    "En revanche, le message contient des lemmes qui apportent une information dans un contexte mais pas dans l’autre (un *posScore* nul par exemple). Dans ce cas-là, la probabilité pondérée que nous avons calculée sera nulle, ce qui aura une incidence beaucoup trop importante sur notre modèle.\n",
    "\n",
    "On peut afficher par exemple les lemmes pour lesquels la probabilité d’apparition en contexte négatif est nulle quand elle existe malgré tout en contexte positif :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ce266-19eb-43dc-9074-8ec5f78bfa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msg[(df_msg['negProb_w'] == 0) & (df_msg['posProb_w'] != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789dd64d-7e02-471d-972b-f806063c021b",
   "metadata": {},
   "source": [
    "Nous allons par conséquent utiliser à nouveau un lissage additif en ajoutant simplement un paramètre $\\alpha$ à toutes les probabilités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce93e1-ecd4-42e2-bb1a-52395dce0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-6\n",
    "\n",
    "df_msg['posProb_w'] = df_msg['posProb_w'] + alpha\n",
    "df_msg['negProb_w'] = df_msg['negProb_w'] + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08e692-5c37-4045-8473-8d6b103706db",
   "metadata": {},
   "source": [
    "#### Étape 3 : Calculer les log-probabilités pondérés par la fréquence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da900c-db58-496f-ae7c-d57045fe54e3",
   "metadata": {},
   "source": [
    "Commencez par importer la bibliothèque *Numpy* qui donne accès à une fonction qui calcule le logarithme naturel :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a28ee-c979-4f12-ae46-0e1a7f9be0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce155d09-c32f-435e-b8df-bdfcd8af59d1",
   "metadata": {},
   "source": [
    "Nous allons dans un premier temps convertir les probabilités en log-probabilités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d96c5-27e5-4d1a-8ae8-e042bb8c2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pos_prob = np.log(df_msg[\"posProb_w\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46413397-cf71-41b4-b288-0b07966eee6d",
   "metadata": {},
   "source": [
    "Puis, dans un second temps, vous enregistrerez dans une variable `log_likelihood_pos` le produit entre les log-probabilités et la fréquence d’occurrence du lemme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207726ec-7eb4-4cc1-963c-8cf2477e92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdf4d2-b639-4012-9d26-a446272aa353",
   "metadata": {},
   "source": [
    "Enfin, ressortez la somme des log-probabilités dans une variable `sum_log_likelihood_pos` grâce à la méthode `.sum()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511ae6e-5715-46dd-a7ae-1b00550e13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8962ee-d5b5-4aba-8468-6d0c60b74ac6",
   "metadata": {},
   "source": [
    "#### Étape 4 : Calculer le MAP score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c8bad-1003-4d33-8645-298c6ef0a60a",
   "metadata": {},
   "source": [
    "En combinant la vraisemblance (*log-likelihood*) et l’a priori (*prior*), vous obtenez le MAP score pour la classe positive :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b0dd4-1d10-4645-83f3-cf1ef2c3cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33afa33-cc4a-4288-8abf-529c7d4574d1",
   "metadata": {},
   "source": [
    "Répétez toutes les étapes pour obtenir le MAP score de la classe négative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ef8fb-0347-4696-b280-4d0efd12905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2eabe-f7ab-4280-8d2f-9fd74a017caf",
   "metadata": {},
   "source": [
    "#### Étape 5 : Calculer le maximum de vraisemblance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d11ad-18ef-4260-b81f-6c06dbe24346",
   "metadata": {},
   "source": [
    "En comparant `score_pos` et `score_neg` pour pourrez déterminer si le message en question est plutôt positif ou négatif :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34077861-2177-4f1a-8e33-5a56644fa608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade05777-4ce9-4439-9f5f-28f1306d779b",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad089901-3fbc-4bd1-ae08-7d86b25a6598",
   "metadata": {},
   "source": [
    "Déçu·es par la prédiction ? Reproduisez toutes les étapes avec le deuxième message, un peu plus nuancé que le premier : vous attendiez-vous à ce résultat ? Quels sont selon vous les facteurs qui peuvent influencer le résultat ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
